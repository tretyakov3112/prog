{"cells":[{"cell_type":"markdown","id":"c4ee07f6","metadata":{"id":"c4ee07f6"},"source":["#  Занятие 12\n","# Полносвязные нейронные сети.\n","\n","Понятие искусственного нейрона. Основные элементы нейронной сети. Функции активации. Обучение нейронной сети (прямое и обратное распространение ошибки).\n"]},{"cell_type":"markdown","id":"df730f75","metadata":{"id":"df730f75"},"source":["В области искусственного интеллекта и машинного обучения **искусственным нейроном** называют вычислительный элемент, представляющий собой математическую модель биологического нейрона, который используется в качестве базового элемента для построения искусственных нейронных сетей.\n","\n","Нейросеть, в которой есть только линейные слои и различные функции активации, называю полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP).\n","\n","Применение нейронной сети к данным (вычисление выхода по заданному входу) часто называют **прямым проходом**, или же **forward propagation (forward pass)**. На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым.\n","\n","![image.png](attachment:image.png)\n","\n","Источник: https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami\n","\n","Нейронные сети состоят из следующих компонентов:\n","\n","* Входной слой, x\n","\n","* Произвольное количество скрытых слоев\n","\n","* Выходной слой, y\n","\n","* Набор весов и смещений между каждым слоем, W и b\n","\n","* Выбор функции активации для каждого скрытого слоя, σ."]},{"cell_type":"markdown","id":"c65d4f39","metadata":{"id":"c65d4f39"},"source":["### Популярные функции активации\n","\n","**ReLU, Rectified linear unit**\n","\n","ReLU представляет собой простую кусочно-линейную функцию. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением.\n","\n","Минусы:\n","* область значений является смещённой относительно нуля;\n","* для отрицательных значений производная равна нулю, что может привести к затуханию градиента.\n","\n","Плюсы:\n","* простота вычисления активации и производной.\n","\n","ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды.\n","\n","**Leaky ReLU**\n","\n","Гиперпараметр обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля.\n","\n","**Sigmoid, сигмоида**\n","\n","Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона.\n","\n","К минусам сигмоиды можно отнести:\n","* область значений смещена относительно нуля;\n","* сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow question 1, question 2;\n","* на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента;\n","* максимальное значение производной составляет 0.25, что также приводит к затуханию градиента.\n","\n","На практике редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM).\n","\n","**Tanh, гиперболический тангенс**\n","\n","Плюсы:\n","* как и сигмоида, имеет ограниченную область значений;\n","* в отличие от сигмоиды, область значений симметрична.\n","\n","Минусы:\n","* требует вычисления экспоненты, что является достаточно сложной вычислительной операцией;\n","* на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента.\n","\n","![image.png](attachment:image.png)\n","\n","Источник: https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami\n","\n","### Для чего нужны функции активации?\n","\n","Линейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою.\n","\n","Добавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Помимо этого, правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами.\n","\n","## Прямое распространение ошибки\n","\n","Рассмотрим как формируется сигнал на выходе нейронной сети.\n","\n","![image-2.png](attachment:image-2.png)\n","\n","![image-3.png](attachment:image-3.png)\n","\n","Источник: https://habr.com/ru/companies/otus/articles/483466/\n","\n","(1) – входной слой\n","\n","(2) – значение нейрона на первом скрытом слое\n","\n","(3) – значение активации на первом скрытом слое\n","\n","(4) – значение нейрона на втором скрытом слое\n","\n","(5) – значение активации на втором скрытом уровне\n","\n","(6) – выходной слой\n","\n","Заключительным шагом в прямом проходе является оценка прогнозируемого выходного значения s относительно ожидаемого выходного значения y. Выходные данные y являются частью обучающего набора данных (x, y), где x – входные данные (как мы помним из предыдущего раздела).\n","\n","Оценка между s и y происходит через функцию потерь, которая характеризует насколько полученное значение близко к ожидаемому. Это может быть, например, среднеквадратичная ошибка.\n","\n","## Обратное распространение ошибки\n","\n","Опираясь на статью 1989 года, метод обратного распространения ошибки:\n","\n","\"Постоянно настраивает веса соединений в сети, чтобы минимизировать меру разности между фактическим выходным вектором сети и желаемым выходным вектором.\n","и\n","…дает возможность создавать полезные новые функции, что отличает обратное распространение от более ранних и простых методов…\"\n","\n","Другими словами, обратное распространение направлено на минимизацию функции потерь путем корректировки весов и смещений сети. Степень корректировки определяется градиентами функции потерь по отношению к этим параметрам.\n","\n","Производная функции потерь отражает чувствительность к изменению значения функции (выходного значения) относительно изменения ее аргумента х (входного значения).\n","\n","Градиент показывает, насколько необходимо изменить параметр x (в положительную или отрицательную сторону), чтобы минимизировать функцию потерь.\n","\n","Вычисление этих градиентов происходит с помощью метода, называемого цепным правилом.\n","\n","![image-4.png](attachment:image-4.png)\n","\n","![image-5.png](attachment:image-5.png)\n","\n","Источник: https://habr.com/ru/companies/otus/articles/483466/\n","\n","Алгоритм оптимизации весов и смещений (также называемый градиентным спуском):\n","\n","* Начальные значения w и b выбираются случайным образом.\n","\n","Эпсилон (e) – это скорость обучения. Он определяет влияние градиента.\n","\n","w и b – матричные представления весов и смещений.\n","\n","* Производная функции потерь по w или b может быть вычислена с использованием частных производных функции потерь по отдельным весам или смещениям.\n","\n","* Условие завершение выполняется, как только функция потерь минимизируется.\n","\n","### Визуальное представление обратного распространения в нейронной сети\n","\n","![image-6.png](attachment:image-6.png)\n","\n","![image-7.png](attachment:image-7.png)\n","Источник: https://habr.com/ru/companies/otus/articles/483466/"]},{"cell_type":"markdown","id":"17f0765c","metadata":{"id":"17f0765c"},"source":["## Практические задания"]},{"cell_type":"markdown","id":"0f8790e1","metadata":{"id":"0f8790e1"},"source":["1. Реализуйте класс, представляющий собой полносвязную нейронную сеть (используйте функцию RELU), которая состоит из входного, скрытого и выходного слоёв. В качестве функции потерь используйте среднеквадратичную ошибку.\n","\n","Для обучения и тестирования класса используйте следующие данные.\n","\n","a =[0, 0, 1, 1, 0, 0,\n","   0, 1, 0, 0, 1, 0,\n","   1, 1, 1, 1, 1, 1,\n","   1, 0, 0, 0, 0, 1,\n","   1, 0, 0, 0, 0, 1]\n","\n","b =[0, 1, 1, 1, 1, 0,\n","   0, 1, 0, 0, 1, 0,\n","   0, 1, 1, 1, 1, 0,\n","   0, 1, 0, 0, 1, 0,\n","   0, 1, 1, 1, 1, 0]\n","\n","c =[0, 1, 1, 1, 1, 0,\n","   0, 1, 0, 0, 0, 0,\n","   0, 1, 0, 0, 0, 0,\n","   0, 1, 0, 0, 0, 0,\n","   0, 1, 1, 1, 1, 0]\n","\n","y =[[1, 0, 0],\n","   [0, 1, 0],\n","   [0, 0, 1]]"]},{"cell_type":"code","source":["import numpy as np\n","\n","class FullyConnectedNeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size):\n","        # Initialize the weights and biases for the input, hidden, and output layers\n","        self.W1 = np.random.randn(input_size, hidden_size)\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size)\n","        self.b2 = np.zeros((1, output_size))\n","\n","    def forward(self, X):\n","        # Compute the output of the hidden layer\n","        Z1 = np.dot(X, self.W1) + self.b1\n","        A1 = np.maximum(0, Z1)  # ReLU activation function\n","\n","        # Compute the output of the output layer\n","        Z2 = np.dot(A1, self.W2) + self.b2\n","        A2 = np.maximum(0, Z2)  # ReLU activation function\n","\n","        return A2\n","\n","    def backward(self, X, Y, A2):\n","        Z1 = np.dot(X, self.W1) + self.b1\n","        A1 = np.maximum(0, Z1)\n","        # Compute the gradient of the loss function with respect to the output layer\n","        dZ2 = A2\n","        dW2 = np.dot(A1.T, dZ2)\n","        db2 = np.sum(dZ2, axis=0, keepdims=True)\n","\n","        # Compute the gradient of the loss function with respect to the hidden layer\n","        dA1 = np.dot(dZ2, self.W2.T)\n","        dZ1 = np.where(A1 > 0, dA1, 0)  # ReLU activation function gradient\n","        dW1 = np.dot(X.T, dZ1)\n","        db1 = np.sum(dZ1, axis=0, keepdims=True)\n","\n","        return dW1, db1, dW2, db2\n","\n","    def train(self, X, Y, num_epochs=1000, learning_rate=0.01):\n","        for epoch in range(num_epochs):\n","            # Forward pass\n","            A2 = self.forward(X)\n","\n","            # Compute the loss function\n","            loss = np.mean((A2) ** 2, axis=0, keepdims=True)\n","\n","            # Backward pass\n","            dW1, db1, dW2, db2 = self.backward(X, Y, A2)\n","\n","            # Update the weights and biases\n","            self.W1 -= learning_rate * dW1\n","            self.b1 -= learning_rate * db1\n","            self.W2 -= learning_rate * dW2\n","            self.b2 -= learning_rate * db2\n","\n","            # Print the loss function every 100 epochs\n","            if epoch % 100 == 0:\n","                print(f\"Epoch {epoch}: loss = {loss}\")\n","\n","\n","# Create the dataset\n","a = np.array([[0, 0, 1, 1, 0, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [1, 1, 1, 1, 1, 1],\n","               [1, 0, 0, 0, 0, 1],\n","               [1, 0, 0, 0, 0, 1]])\n","b = np.array([[0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [0, 1, 1, 1, 1, 0]])\n","c = np.array([[0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 1, 1, 1, 0]])\n","y = np.array([[1, 0, 0],\n","               [0, 1, 0],\n","               [0, 0, 1]])\n","\n","# Create the neural network\n","nn = FullyConnectedNeuralNetwork(6, 4, 3)\n","\n","# Train the neural network\n","nn.train(np.concatenate((a, b, c), axis=0), y)\n","\n","# Test the neural network\n","print(nn.forward(np.array([[0, 0, 1, 1, 0, 0]])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_HgTRjfQhw0","executionInfo":{"status":"ok","timestamp":1714724077598,"user_tz":-180,"elapsed":429,"user":{"displayName":"Александр Третьяков","userId":"06557649025736691109"}},"outputId":"6e802e0e-ff9b-4281-a4e5-caf32da5a75d"},"id":"M_HgTRjfQhw0","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: loss = [[35.68455826  5.84870738  0.35222482]]\n","Epoch 100: loss = [[0.00000000e+00 2.90468942e-07 9.03244007e-05]]\n","Epoch 200: loss = [[0.00000000e+00 4.90050391e-10 1.59935250e-07]]\n","Epoch 300: loss = [[0.00000000e+00 9.10586522e-13 2.97791377e-10]]\n","Epoch 400: loss = [[0.00000000e+00 1.69893651e-15 5.55656438e-13]]\n","Epoch 500: loss = [[0.00000000e+00 3.17036801e-18 1.03690882e-15]]\n","Epoch 600: loss = [[0.00000000e+00 5.91622856e-21 1.93497995e-18]]\n","Epoch 700: loss = [[0.00000000e+00 1.10395787e-23 3.61088186e-21]]\n","Epoch 800: loss = [[0.00000000e+00 2.05835520e-26 6.73820902e-24]]\n","Epoch 900: loss = [[0.00000000e+00 3.87236206e-29 1.25819773e-26]]\n","[[0.00000000e+00 0.00000000e+00 1.94289029e-14]]\n"]}]},{"cell_type":"markdown","id":"532aca25","metadata":{"id":"532aca25"},"source":["2. Рассмотрите несколько вариантов инициализации весов и смещений в нейроннной сети из п. 1. Какой вариант инициализации оказался наилучшим?"]},{"cell_type":"code","source":["import numpy as np\n","\n","class FullyConnectedNeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size):\n","        # Initialize the weights and biases for the input, hidden, and output layers\n","        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n","        self.b2 = np.zeros((1, output_size))\n","\n","\n","    def forward(self, X):\n","        # Compute the output of the hidden layer\n","        Z1 = np.dot(X, self.W1) + self.b1\n","        A1 = np.maximum(0, Z1)  # ReLU activation function\n","\n","        # Compute the output of the output layer\n","        Z2 = np.dot(A1, self.W2) + self.b2\n","        A2 = np.maximum(0, Z2)  # ReLU activation function\n","\n","        return A2\n","\n","    def backward(self, X, Y, A2):\n","        Z1 = np.dot(X, self.W1) + self.b1\n","        A1 = np.maximum(0, Z1)\n","        # Compute the gradient of the loss function with respect to the output layer\n","        dZ2 = A2\n","        dW2 = np.dot(A1.T, dZ2)\n","        db2 = np.sum(dZ2, axis=0, keepdims=True)\n","\n","        # Compute the gradient of the loss function with respect to the hidden layer\n","        dA1 = np.dot(dZ2, self.W2.T)\n","        dZ1 = np.where(A1 > 0, dA1, 0)  # ReLU activation function gradient\n","        dW1 = np.dot(X.T, dZ1)\n","        db1 = np.sum(dZ1, axis=0, keepdims=True)\n","\n","        return dW1, db1, dW2, db2\n","\n","    def train(self, X, Y, num_epochs=1000, learning_rate=0.01):\n","        for epoch in range(num_epochs):\n","            # Forward pass\n","            A2 = self.forward(X)\n","\n","            # Compute the loss function\n","            loss = np.mean((A2) ** 2, axis=0, keepdims=True)\n","\n","            # Backward pass\n","            dW1, db1, dW2, db2 = self.backward(X, Y, A2)\n","\n","            # Update the weights and biases\n","            self.W1 -= learning_rate * dW1\n","            self.b1 -= learning_rate * db1\n","            self.W2 -= learning_rate * dW2\n","            self.b2 -= learning_rate * db2\n","\n","            # Print the loss function every 100 epochs\n","            if epoch % 100 == 0:\n","                print(f\"Epoch {epoch}: loss = {loss}\")\n","\n","\n","# Create the dataset\n","a = np.array([[0, 0, 1, 1, 0, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [1, 1, 1, 1, 1, 1],\n","               [1, 0, 0, 0, 0, 1],\n","               [1, 0, 0, 0, 0, 1]])\n","b = np.array([[0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 1, 0],\n","               [0, 1, 1, 1, 1, 0]])\n","c = np.array([[0, 1, 1, 1, 1, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 0, 0, 0, 0],\n","               [0, 1, 1, 1, 1, 0]])\n","y = np.array([[1, 0, 0],\n","               [0, 1, 0],\n","               [0, 0, 1]])\n","\n","# Create the neural network\n","nn = FullyConnectedNeuralNetwork(6, 4, 3)\n","\n","# Train the neural network\n","nn.train(np.concatenate((a, b, c), axis=0), y)\n","\n","# Test the neural network\n","print(nn.forward(np.array([[0, 0, 1, 1, 0, 0]])))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b54n-Y_ViE3d","executionInfo":{"status":"ok","timestamp":1714724923294,"user_tz":-180,"elapsed":429,"user":{"displayName":"Александр Третьяков","userId":"06557649025736691109"}},"outputId":"f9cfbd8f-b6f0-490e-a04e-656b19baea95"},"id":"b54n-Y_ViE3d","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: loss = [[0.24525609 0.         0.        ]]\n","Epoch 100: loss = [[1.04536455e-11 0.00000000e+00 0.00000000e+00]]\n","Epoch 200: loss = [[6.26278865e-21 0.00000000e+00 0.00000000e+00]]\n","Epoch 300: loss = [[3.63641253e-30 0.00000000e+00 0.00000000e+00]]\n","Epoch 400: loss = [[4.10865055e-33 0.00000000e+00 0.00000000e+00]]\n","Epoch 500: loss = [[4.10865055e-33 0.00000000e+00 0.00000000e+00]]\n","Epoch 600: loss = [[1.02716264e-33 0.00000000e+00 0.00000000e+00]]\n","Epoch 700: loss = [[1.02716264e-33 0.00000000e+00 0.00000000e+00]]\n","Epoch 800: loss = [[0. 0. 0.]]\n","Epoch 900: loss = [[0. 0. 0.]]\n","[[0. 0. 0.]]\n"]}]},{"cell_type":"markdown","id":"e27c10c2","metadata":{"id":"e27c10c2"},"source":["3. Проанализируйте процесс обучения модели.\n","\n","* Постройте графики зависимости функции потерь от номера эпохи обучения для различных значений скорости обучения.\n","\n","* Постройте графики зависимости функции потерь от номера эпохи обучения для различных функций активации (рассмотрете не менее 3-х функций активации)."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}